---
title: "Problem Set 1"
author: "Thomas Kelley-Kemple, Joanna Moody, Vinh Nyugen, Joanna"
date: "September 18, 2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(haven)
library(psych)
library(plyr)
library(reshape2)
library(lme4)
library(tidyr)
```

**Part 1**

1) \textbf{On the basis of this correlation, the researcher states that the reliability of the ratings is 0.7.  What score is she assuming is relevant, that has a reliability of 0.7? Is it the score from e-Rater A? The score from e-Rater B? Some combination of both scores? A score from any single e-Rater? Or other?}

2) \textbf{If the researcher considers 0.7 to be the reliability, what is the replication that she assumes is relevant?  What is random, and what is fixed?}

3) \textbf{Apply Spearman-Brown (actually perform a calculation) to estimate the reliability of the average these two e-Rater scores.}

4) \textbf{The company asks you what a valid use of the third score would be. Remember this score is available for only 10\% of examinees. Should the company use this score alone? Should it use the unweighted average of the three scores (the two e-rater scores and the human score)?  Should it ignore the score and use the average of the two e-Rater scores?  Answer the following questions:}
    a) \textbf{If you were an examinee with a high (well above average) true score, would you rather have the human score, the average of the two e-rater scores, or the average of all three scores?}
    
    b) \textbf{If you were an examinee with a low (well below average) true score, would you rather have the human score, the average of the two e-rater scores, or the average of all three scores? }
  
    c) \textbf{Weighing all considerations for the intended use of these scores for college admissions, what would be your recommendation to the company for how they should use this third score?}
  
**Part 2**
```{r,echo=T}
#Read in data
data_raw<- read_dta("./Assignment1.dta")
```

5) \textbf{Using Stata, calculate coefficient alpha for the first occasion and the second occasion separately.   In a sentence or two, interpret coefficient alpha for the first occasion (see also Question 16).}
```{r,echo=T}
#create variable lists
o1 <- paste0("x_o1_i", 1:12)
o2 <- paste0("x_o2_i", 1:12)
#subset data
data_o1 <- subset(data_raw, select = o1)
data_o2 <- subset(data_raw, select = o2)
#create alpha output
alpha1 <- alpha(data_o1, keys=NULL, title=NULL, cumulative=FALSE, max=10, na.rm = TRUE, check.keys=TRUE, n.iter=1, delete=TRUE)
alpha2 <- alpha(data_o2, keys=NULL, title=NULL, cumulative=FALSE, max=10, na.rm = TRUE, check.keys=TRUE, n.iter=1, delete=TRUE)
#get just the alpha numbers
alpha_time1 <- alpha1$total$std.alpha
alpha_time2 <- alpha2$total$std.alpha
#output alpha data
alpha1
alpha2
```

For this sample $\alpha_1 = `r round(alpha_time1,3)`$ and $\alpha_2 = `r round(alpha_time2,3)`$

6) \textbf{Using Stata, calculate the average score of participants from the first occasion, then calculate the average score of participants from the second occasion. Then, calculate the correlation between the two average scores using code like pwcorr avgscr1 avgscr2.  Report this correlation and, in a sentence or two, provide an interpretation (see also Question 16).}
```{r}
avg_o1 <- rowMeans(data_o1)
avg_o2 <- rowMeans(data_o2)

cor(avg_o1, avg_o2)
```

7) \textbf{ Reload the data and reshape it for analysis in Stata.  Although it is a pain, I am requiring you to use some of the code that we have presented in the past .do files to reshape the data from “double-wide” format.  See, for example, the Class03.do and Class04.do files.  As one way to check your work, submit a screenshot of the output from code like table person item occasion, contents(mean score) and/or simply table person item occasion .}

```{r,echo=T}

data_raw$person <- factor(data_raw$person)
colnames(data_raw) <- c("person", paste0("1_",1:12), paste0("2_",1:12))

data_long <- melt(data_raw, id.vars=c("person"))
data_long <- separate(data = data_long, col = variable, into = c("occasion", "item"), sep = "_")
```

8) \textbf{8. Note the code available to you in the .do files, and include a) a discrete histogram of all 25x12x2 scores, b) a histogram of marginal person scores, c) a histogram of marginal item scores, and d) a histogram of marginal occasion scores.  Use discrete histograms where you think they are appropriate, or substitute tables if histograms are not informative, for example, tabulate occasion, summarize(score) .  Histograms of interactions are not necessary.}


**Part 3**

9) \textbf{Write out the model implied by the data collection design under the tenets of Generalizability Theory.  Draw the Venn diagram for this design.}

The model implied here can be written as
\begin{align*}
X_{pi}&=\mu + \nu_p + \nu_i +\nu_o+ \nu_{pi}+ \nu_{po}+ \nu_{oi}+ \nu_{pio,e}\\
\nu_p&\sim N(0,\sigma^2_p)\\
\nu_i&\sim N(0,\sigma^2_i)\\
\nu_o&\sim N(0,\sigma^2_o)\\
\nu_{pi}&\sim N(0,\sigma^2_{pi})\\
\nu_{po}&\sim N(0,\sigma^2_{po})\\
\nu_{io}&\sim N(0,\sigma^2_{io})\\
\nu_{pio,e}&\sim N(0,\sigma^2_{pio,e})
\end{align*}
The ven diagram for the variances is seen below:

```{r,echo=F,fig.align='center',fig.width=5,fig.height=3}
knitr::include_graphics('./VennDiagram.jpg')
```

10) \textbf{Estimate the variance components for this model using the mixed or xtmixed command. Feel free to go get coffee while this runs.  Don’t forget to create interactions using commands like egen pXi = group(person item).  Include a table with four columns, the source of variance, the estimated variance components, their square roots, and their percentage of total score variance.}

```{r,echo=T}

data_long$pxi <- as.factor(100*as.numeric(data_long$person)+as.numeric(data_long$item))
data_long$pxo <- as.factor(100*as.numeric(data_long$person)+as.numeric(data_long$occasion))
data_long$oxi <- as.factor(10*as.numeric(data_long$occasion)+as.numeric(data_long$item))

mixed <- lmer(value ~ 1 + (1|person) + (1|item) + (1|occasion) + (1|pxi) + 
                (1|pxo) + (1|oxi) ,data=data_long)

summary(mixed)
```

11) \textbf{A novice psychometrician with no sense of the context observes from the percentages, “it looks like items are a much greater source of variance than occasions!”  Explain the flaw in this reasoning.}

12) \textbf{Estimate the Mean Squares for this model using the anova command.  You will first need to set the maximum matrix size to a large number, using code like set matsize 1000. Write out the equation for the estimated variance component, $\hat{\sigma^2_p}$, in terms of mean squares, $MS$, and confirm that this calculation corresponds to your results from mixed or xtmixed.  Recall that $n_p =25, n_i=12$ and $n_o=2$.}

```{r}
anovlm <- lm(value ~ person + item + occasion +
               pxi + pxo + oxi,data=data_long )

anova(anovlm)
```

13) \textbf{Calculate and report the mean and standard deviation of marginal person scores, averaging over items and occasions.  Following the code from class, you could obtain this using code like, summarize pmean if ptag .  Explain why the term $\hat{\sigma_p}$, is less than the standard deviation of marginal person means.}

14) \textbf{Describe the $o$, $po$, and $io$ variance components in words, and include whether they are good, bad, or neutral with respect to relative error in a $p \times i \times o$ design.   There is no need to reference the actual values, here.}

The $o$ variance component describes variance across occations. 

**Part 4**

15) \textbf{Write out the full equation for the relative error variance, $\sigma^2_\delta$}

16) \textbf{Calculate the generalizability coefficient for relative error, $E\hat{\rho}^2$, when there are 12 items administered on one occasion.  Explain the differences between this coefficient, the coefficients from Question 5, and the coefficient from Question 6.  Explain the differences between the questions that these different coefficients answer.}

17) \textbf{Use the “pxixr D Study Template” to include a graph of a) the standard error of measurement and b) the generalizability coefficient for relative error.  Relabel and rescale where appropriate.}

18) \textbf{If the scale is administered on 1 occasion, how many items are required to achieve a reliability of 0.75? You can use the template to answer this.}

19) \textbf{Compare the benefits of doubling the number of items from 6 to 12 versus doubling the number of occasions from 1 to 2.  Compare the benefits of doubling the number of items from 12 to 24 versus doubling the number of occasions from 1 to 2.  How could you use this information to address the question of whether items are a greater source of error than occasions?}

 